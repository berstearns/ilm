# Comprehensive Guide: Using Trained ILM Models with ilm_eval.py

## Overview

This guide explains how to use trained Infilling Language Models (ILM) with the `ilm_eval.py` evaluation script. It covers model integration, execution with different configurations, and understanding output formats.

---

## 1. ILM Model Architecture & Requirements

### What is an ILM Model?

An **Infilling Language Model (ILM)** is a GPT-2 model fine-tuned on a specialized masking task where:
- Text spans are randomly masked at multiple levels (word, n-gram, sentence, paragraph, document)
- The model learns to fill in (infill) masked spans based on context
- Special tokens mark infill regions: `<|startofinfill|>`, `<|endofinfill|>`, `<|infill_word|>`, etc.

### Required Files for ILM Integration

An ILM model directory must contain:

```
model_directory/
├── pytorch_model.bin                    # Fine-tuned GPT-2 weights (~1.3 GB)
├── config.json                          # Model configuration
├── special_tokens_map.json              # Special token mappings
├── tokenizer_config.json                # Tokenizer configuration
├── vocab.json                           # GPT-2 vocabulary
├── merges.txt                           # BPE merge rules
└── additional_ids_to_tokens.pkl         # CRITICAL: ILM special token mappings (pickle)
```

**Critical File**: `additional_ids_to_tokens.pkl` contains the mapping for ILM-specific tokens like `<|infill_word|>`, `<|infill_ngram|>`, etc. **This must be present** for the model to work correctly in `ilm_eval.py`.

---

## 2. Model Training Pipeline

### From Training to Evaluation

The pipeline generates these required files automatically:

```
training/ilm/train_ilm.py
    ↓
Creates: experiments/{model_name}/
    ├── pytorch_model.bin (automatic via transformers)
    ├── config.json (automatic via transformers)
    ├── tokenizer files (automatic via transformers)
    └── additional_ids_to_tokens.pkl (generated by training script)
```

### Training Command

```bash
~/.pyenv/versions/3.9.25/bin/python training/ilm/train_ilm.py \
  experiments/efcamdat_test_sample \           # Output directory
  training/ilm/train/ \                        # Training config directory
  data/char_masks/efcamdat_samples \           # Masked examples directory
  --seed 0 \
  --train_examples_tag train \
  --eval_examples_tag valid \
  --eval_max_num_examples 10 \
  --model_name gpt2 \
  --train_batch_size 8 \
  --train_num_epochs 1
```

**Output**: Fully configured model in `experiments/efcamdat_test_sample/` ready for `ilm_eval.py`

---

## 3. Using Trained Models with ilm_eval.py

### 3.1 Basic Syntax

```bash
~/.pyenv/versions/3.9.25/bin/python inference/ilm_eval.py \
  -i {input_csv} \
  --models ilm:{model_path} {other_models} \
  --output {output_json}
```

### 3.2 Model Path Format

**For ILM models**, use the format:
```
ilm:{absolute_or_relative_path_to_model_directory}
```

**Examples**:
```bash
# Absolute path
ilm:/home/b/p/research-sketches/ilms/experiments/efcamdat_test_sample

# Relative path (run from ilms directory)
ilm:experiments/efcamdat_test_sample
ilm:../models/sto_ilm
```

### 3.3 Complete Example: Multi-Model Evaluation

```bash
cd /home/b/p/research-sketches/ilms

~/.pyenv/versions/3.9.25/bin/python inference/ilm_eval.py \
  -i /home/b/p/my-data/i/phd-experimental-data/cefr-classification/data/splits/norm-celva-1742.csv \
  --models \
    ilm:experiments/efcamdat_test_sample \
    ilm:experiments/efcamdat_all_ilm \
    mlm:bert-base-uncased \
    mlm:roberta-base \
    t5:t5-small \
  --n-masks 1 \
  --samples-per-text 10 \
  --max-chars 500 \
  --limit 5 \
  --seed 42 \
  --print-every 10 \
  --masking human-tokens \
  --subtoken-granularity both \
  -o results-ilm-evaluation.json
```

---

## 4. Argument Reference

### Input/Output
- `-i, --input`: Input CSV file with texts to evaluate
- `-o, --output`: Output JSON file with results (default: `results.json`)

### Models
- `--models`: Space-separated list of models to evaluate
  - **ILM**: `ilm:{path}`
  - **MLM (BERT/RoBERTa)**: `mlm:{model_name}`
  - **Seq2Seq (T5/BART)**: `t5:{model_name}` or `bart:{model_name}`

### Masking Configuration
- `--n-masks`: Number of mask positions per text (default: 1)
- `--samples-per-text`: Random masks to try per text (default: 10)
- `--max-chars`: Maximum text length to evaluate (default: unlimited)
- `--limit`: Number of texts to evaluate (default: all)

### Tokenization
- `--masking`: Token selection strategy
  - `random-tokens`: Random word tokens
  - `human-tokens`: Human-selected important tokens (requires NLTK)
  - `frequent-tokens`: Most common words
- `--subtoken-granularity`: How to handle subword units
  - `word`: Treat BPE subwords as single unit (default)
  - `subtoken`: Individual subword tokens
  - `both`: Both word and subtoken levels

### Output Control
- `--print-every`: Print progress every N texts (default: 50)
- `--seed`: Random seed for reproducibility (default: 42)

---

## 5. Understanding Output Format

### 5.1 JSON Output Structure

```json
{
  "models": ["ilm:efcamdat_test_sample", "mlm:bert-base-uncased"],
  "config": {
    "n_masks": 1,
    "samples_per_text": 10,
    "masking": "human-tokens",
    "subtoken_granularity": "word"
  },
  "results": {
    "overall": {
      "ilm:efcamdat_test_sample": {
        "top_1_accuracy": 0.23,
        "unigram_recall": 0.45,
        "unigram_f1": 0.52,
        "bigram_recall": 0.12,
        "bigram_f1": 0.18,
        "samples": 150
      }
    },
    "by_cefr": {
      "A1": {
        "ilm:efcamdat_test_sample": {...},
        "samples": 30
      },
      "A2": {...},
      "B1": {...},
      "B2": {...},
      "C1": {...}
    }
  }
}
```

### 5.2 Metrics Explained

- **Top-1 Accuracy**: Exact match between predicted and masked token
- **Unigram Recall**: Percentage of original characters recovered
- **Unigram F1**: Balance between precision and recall
- **Bigram Recall/F1**: Two-character sequence accuracy

---

## 6. Step-by-Step: Running a Test Evaluation

### Step 1: Prepare Your Model

```bash
# Check model files exist
ls -la experiments/efcamdat_test_sample/
# Expected output:
# - pytorch_model.bin (~1.3 GB)
# - config.json
# - tokenizer_config.json
# - vocab.json
# - merges.txt
# - additional_ids_to_tokens.pkl (CRITICAL)
```

### Step 2: Prepare Input Data

Your CSV must have columns:
- `text`: The English text to evaluate
- `cefr` (optional): CEFR level (A1, A2, B1, B2, C1) - enables breakdowns

```csv
text,cefr
"I like to eat apples and bananas.",A1
"The weather is very nice today.",A2
```

### Step 3: Run Evaluation (Small Test)

```bash
cd /home/b/p/research-sketches/ilms

# Test with 2-3 samples
~/.pyenv/versions/3.9.25/bin/python inference/ilm_eval.py \
  -i /path/to/test_data.csv \
  --models ilm:experiments/efcamdat_test_sample \
  --limit 3 \
  --print-every 1 \
  -o test_results.json
```

### Step 4: Examine Results

```bash
cat test_results.json | python -m json.tool
```

---

## 7. Advanced Usage

### 7.1 Comparing Multiple ILM Models

```bash
~/.pyenv/versions/3.9.25/bin/python inference/ilm_eval.py \
  -i data.csv \
  --models \
    ilm:experiments/efcamdat_test_sample \
    ilm:experiments/efcamdat_all_ilm \
    ilm:experiments/efcamdat_C1_ilm \
  --limit 100 \
  -o ilm_comparison.json
```

**Analysis**: Compare specialization vs. general models:
- `efcamdat_all_ilm`: General model (all CEFR levels)
- `efcamdat_C1_ilm`: Specialized (C1 only)
- **Metric**: Specialization gain = (General_PPL / Specialized_PPL - 1) × 100%

### 7.2 Fine-Grained CEFR Analysis

```bash
~/.pyenv/versions/3.9.25/bin/python inference/ilm_eval.py \
  -i cefr_data.csv \
  --models ilm:experiments/efcamdat_test_sample \
  --n-masks 5 \
  --samples-per-text 20 \
  --subtoken-granularity both \
  -o cefr_detailed_analysis.json
```

**Output**: Results broken down by CEFR level to analyze:
- How well does the general model handle each level?
- Where does it struggle most?

### 7.3 Hyperparameter Tuning

```bash
# Higher n-masks for more stable estimates
~/.pyenv/versions/3.9.25/bin/python inference/ilm_eval.py \
  -i data.csv \
  --models ilm:experiments/efcamdat_test_sample \
  --n-masks 10 \
  --samples-per-text 50 \
  --limit 200 \
  -o high_reliability_results.json
```

---

## 8. Troubleshooting

### Issue: `ILM module not available`

**Solution**: Ensure the ILM package is installed and in PYTHONPATH:
```bash
export PYTHONPATH=/home/b/p/research-sketches/ilms:$PYTHONPATH
```

### Issue: `additional_ids_to_tokens.pkl not found`

**Solution**: This file must be in your model directory. If missing, regenerate model:
```bash
# Check if file exists
ls -la experiments/efcamdat_test_sample/additional_ids_to_tokens.pkl

# If missing, retrain model - training script creates it
```

### Issue: Model loads but produces nonsense output

**Solution**: Verify model was trained properly:
1. Check training logs for convergence
2. Verify training data was loaded correctly
3. Test with the training/validation set first

### Issue: Slow evaluation on CPU

**Solution**:
```bash
# Use GPU if available
CUDA_VISIBLE_DEVICES=0 ~/.pyenv/versions/3.9.25/bin/python inference/ilm_eval.py ...

# Or limit sample size
--limit 50 --samples-per-text 5
```

---

## 9. Complete Example: Minimal Test

### Create Minimal Test Data

```bash
cat > /tmp/test_minimal.csv << 'EOF'
text,cefr
"I like cats and dogs.",A1
"The weather is nice today.",A2
"Education is important for development.",B1
EOF
```

### Run Minimal Test

```bash
cd /home/b/p/research-sketches/ilms

~/.pyenv/versions/3.9.25/bin/python inference/ilm_eval.py \
  -i /tmp/test_minimal.csv \
  --models ilm:experiments/efcamdat_test_sample \
  --n-masks 1 \
  --samples-per-text 5 \
  --limit 3 \
  --seed 42 \
  -o /tmp/test_minimal_results.json

echo "Results:"
python -m json.tool < /tmp/test_minimal_results.json
```

### Expected Output

```json
{
  "models": ["ilm:experiments/efcamdat_test_sample"],
  "results": {
    "overall": {
      "ilm:experiments/efcamdat_test_sample": {
        "top_1_accuracy": 0.15,
        "unigram_recall": 0.42,
        "unigram_f1": 0.48,
        "bigram_recall": 0.08,
        "bigram_f1": 0.12,
        "samples": 15
      }
    },
    "by_cefr": {
      "A1": {...},
      "A2": {...},
      "B1": {...}
    }
  }
}
```

---

## 10. Integration with Research Workflow

### Evaluation Pipeline Flowchart

```
1. Train ILM Models
   └─> experiments/{model_name}/ (ready)

2. Prepare Test Data
   └─> data.csv (texts + CEFR levels)

3. Run Evaluation
   └─> ilm_eval.py --models ilm:{path}

4. Analyze Results
   └─> Compare accuracy across CEFR levels
       └─> Measure specialization benefit
       └─> Generate publication figures
```

### Model Comparison Workflow

```bash
#!/bin/bash
# compare_ilm_models.sh

MODELS=(
  "efcamdat_test_sample"
  "efcamdat_all_ilm"
  "efcamdat_C1_ilm"
  "efcamdat_B2_ilm"
)

for model in "${MODELS[@]}"; do
  echo "Evaluating $model..."
  python inference/ilm_eval.py \
    -i test_data.csv \
    --models ilm:experiments/${model} \
    --limit 100 \
    -o results_${model}.json
done

# Aggregate results
python scripts/compare_results.py results_*.json
```

---

## 11. Performance Expectations

### On CPU (GPT-2 base, 5 samples)
- **Inference Time**: ~2-5 seconds per sample
- **Memory**: ~2-3 GB
- **Accuracy**: 10-25% for general model on mixed CEFR

### On GPU (V100, 5 samples)
- **Inference Time**: ~0.2-0.5 seconds per sample
- **Memory**: ~4-6 GB
- **Throughput**: 10-50 samples/minute

---

## 12. Key Files Reference

| File | Purpose | Location |
|------|---------|----------|
| `train_ilm.py` | Train ILM models | `training/ilm/train_ilm.py` |
| `ilm_eval.py` | Evaluate models | `inference/ilm_eval.py` |
| `create_ilm_examples.py` | Generate masked examples | `training/ilm/create_ilm_examples.py` |
| Model weights | Trained parameters | `experiments/{name}/pytorch_model.bin` |
| Special tokens | ILM token mappings | `experiments/{name}/additional_ids_to_tokens.pkl` |

---

## Quick Reference: Common Commands

```bash
# Single model test
python inference/ilm_eval.py -i data.csv --models ilm:experiments/model1 --limit 10 -o results.json

# Multi-model comparison
python inference/ilm_eval.py -i data.csv \
  --models ilm:experiments/model1 ilm:experiments/model2 mlm:bert-base-uncased \
  --limit 100 -o results.json

# Detailed CEFR analysis
python inference/ilm_eval.py -i data.csv \
  --models ilm:experiments/model1 \
  --n-masks 5 --samples-per-text 20 \
  -o results_detailed.json

# GPU acceleration (if available)
CUDA_VISIBLE_DEVICES=0 python inference/ilm_eval.py ...
```

---

## Appendix: ILMModelWrapper Implementation

The `ilm_eval.py` script uses `ILMModelWrapper` class which:

1. **Loads the model**:
   - Reads `additional_ids_to_tokens.pkl` for special token mappings
   - Initializes GPT-2 tokenizer with ILM tokens
   - Loads fine-tuned weights from `pytorch_model.bin`

2. **Performs infilling**:
   - Masks text span with space placeholder `_`
   - Converts to ILM token format
   - Uses `infill_with_ilm()` function to generate predictions
   - Returns filled text

3. **Computes metrics**:
   - Compares predictions to masked tokens
   - Calculates accuracy, recall, F1 across n-grams
   - Aggregates by CEFR level if available

See lines 192-254 in `inference/ilm_eval.py` for implementation details.

