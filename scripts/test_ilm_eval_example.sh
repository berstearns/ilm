#!/bin/bash

##############################################################################
# TEST SCRIPT: Minimal ILM Model Evaluation with ilm_eval.py
#
# This script demonstrates:
# 1. Creating minimal test data (2-3 samples)
# 2. Running ilm_eval.py with trained ILM model
# 3. Verifying output format and metrics
#
# Usage: bash test_ilm_eval_example.sh
##############################################################################

set -e

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  ILM Model Evaluation Test - Comprehensive End-to-End Demo        â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Configuration
PROJECT_ROOT="/home/b/p/research-sketches/ilms"
PYTHON_BIN="~/.pyenv/versions/3.9.25/bin/python"
TEST_DATA_CSV="/tmp/ilm_test_data.csv"
TEST_OUTPUT_JSON="/tmp/ilm_test_results.json"
MODEL_PATH="experiments/efcamdat_test_sample"

echo "ðŸ“‹ Step 1: Creating test data with 3 samples (2-3 CEFR levels)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Create minimal test CSV with CEFR levels
cat > "$TEST_DATA_CSV" << 'EOF'
text,cefr
"I like to eat apples and bananas with my friends.",A1
"The weather is very nice today and I enjoy walking in the park.",A2
"Education plays a crucial role in the development of individuals and society.",B1
EOF

echo "âœ“ Test data created: $TEST_DATA_CSV"
echo ""
echo "Content:"
cat "$TEST_DATA_CSV"
echo ""

echo ""
echo "ðŸ“ Step 2: Verifying model files"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

cd "$PROJECT_ROOT" || exit 1

echo "Checking model directory: $MODEL_PATH"
echo ""

if [ -d "$MODEL_PATH" ]; then
    echo "âœ“ Model directory exists"
    echo ""
    echo "ðŸ“¦ Files in model directory:"
    ls -lh "$MODEL_PATH"/ | grep -E "pytorch_model|additional_ids|config|tokenizer|vocab|merges"
    echo ""

    # Check critical file
    if [ -f "$MODEL_PATH/additional_ids_to_tokens.pkl" ]; then
        echo "âœ“ CRITICAL: additional_ids_to_tokens.pkl found"
    else
        echo "âš  WARNING: additional_ids_to_tokens.pkl NOT found"
        echo "  This file is required for ILM evaluation"
    fi
else
    echo "âœ— ERROR: Model directory not found at $MODEL_PATH"
    exit 1
fi

echo ""

echo ""
echo "ðŸš€ Step 3: Running ilm_eval.py with minimal configuration"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "Command:"
echo "  ~/.pyenv/versions/3.9.25/bin/python inference/ilm_eval.py \\"
echo "    -i $TEST_DATA_CSV \\"
echo "    --models ilm:$MODEL_PATH \\"
echo "    --limit 3 \\"
echo "    --n-masks 1 \\"
echo "    --samples-per-text 5 \\"
echo "    --print-every 1 \\"
echo "    --seed 42 \\"
echo "    -o $TEST_OUTPUT_JSON"
echo ""

# Run evaluation (using ~/.pyenv/versions... won't work in subshell, so use full path)
eval "$PYTHON_BIN inference/ilm_eval.py \
  -i $TEST_DATA_CSV \
  --models ilm:$MODEL_PATH \
  --limit 3 \
  --n-masks 1 \
  --samples-per-text 5 \
  --print-every 1 \
  --seed 42 \
  -o $TEST_OUTPUT_JSON"

echo ""
echo "âœ“ Evaluation completed!"
echo ""

echo ""
echo "ðŸ“Š Step 4: Examining results"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

if [ -f "$TEST_OUTPUT_JSON" ]; then
    echo "âœ“ Results file created: $TEST_OUTPUT_JSON"
    echo ""
    echo "ðŸ“‹ Results (formatted JSON):"
    echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
    eval "$PYTHON_BIN" -m json.tool < "$TEST_OUTPUT_JSON"
    echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
else
    echo "âœ— ERROR: Results file not created"
    exit 1
fi

echo ""

echo ""
echo "ðŸ“ˆ Step 5: Key metrics extraction"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# Extract and display key metrics using Python
eval "$PYTHON_BIN" << 'PYTHON_SCRIPT'
import json
import sys

try:
    with open('/tmp/ilm_test_results.json', 'r') as f:
        results = json.load(f)

    print("âœ“ Successfully parsed results JSON")
    print()

    # Overall metrics
    if 'results' in results and 'overall' in results['results']:
        overall = results['results']['overall']

        for model_name, metrics in overall.items():
            print(f"Model: {model_name}")
            print("â”œâ”€ Accuracy (Top-1):   {:.1%}".format(metrics.get('top_1_accuracy', 0)))
            print("â”œâ”€ Unigram Recall:     {:.1%}".format(metrics.get('unigram_recall', 0)))
            print("â”œâ”€ Unigram F1:         {:.1%}".format(metrics.get('unigram_f1', 0)))
            print("â”œâ”€ Bigram Recall:      {:.1%}".format(metrics.get('bigram_recall', 0)))
            print("â”œâ”€ Bigram F1:          {:.1%}".format(metrics.get('bigram_f1', 0)))
            print("â””â”€ Samples Evaluated:  {}".format(metrics.get('samples', 0)))
            print()

    # By CEFR breakdown
    if 'results' in results and 'by_cefr' in results['results']:
        by_cefr = results['results']['by_cefr']

        print("Breakdown by CEFR Level:")
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        for cefr_level in sorted(by_cefr.keys()):
            cefr_data = by_cefr[cefr_level]
            if isinstance(cefr_data, dict) and 'samples' in cefr_data:
                samples = cefr_data.get('samples', 0)
                print(f"  {cefr_level}: {samples} samples")

                # Get model metrics if available
                for model_name, metrics in cefr_data.items():
                    if model_name != 'samples' and isinstance(metrics, dict):
                        accuracy = metrics.get('top_1_accuracy', 0)
                        print(f"    â””â”€ {model_name}: {accuracy:.1%} accuracy")
        print()

    print("âœ“ Metrics successfully extracted")

except Exception as e:
    print(f"âœ— Error parsing results: {e}")
    sys.exit(1)

PYTHON_SCRIPT

echo ""

echo ""
echo "âœ… TEST COMPLETE"
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  Success! ILM model evaluation works end-to-end                  â•‘"
echo "â•‘                                                                  â•‘"
echo "â•‘  Next Steps:                                                     â•‘"
echo "â•‘  1. Evaluate on full dataset: --limit 100                        â•‘"
echo "â•‘  2. Run multi-model comparison (general + specialized)           â•‘"
echo "â•‘  3. Analyze CEFR-level breakdowns                                â•‘"
echo "â•‘                                                                  â•‘"
echo "â•‘  For details, see: ILM_MODEL_EVALUATION_GUIDE.md                 â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

echo "ðŸ“ Output files:"
echo "   Test data:   $TEST_DATA_CSV"
echo "   Results:     $TEST_OUTPUT_JSON"
echo ""
echo "To view results again: python -m json.tool < $TEST_OUTPUT_JSON"
echo ""

